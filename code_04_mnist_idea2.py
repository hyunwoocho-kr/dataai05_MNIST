# -*- coding: utf-8 -*-
"""code 04_mnist_Idea2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E5sQr_eqSK06WiV_6v1rmlKG0v00oIoj
"""

#hide
!pip install -Uqq fastbook
#import fastbook
#fastbook.setup_book()

#hide
from fastbook import *
from fastai.vision.all import *

path = untar_data(URLs.MNIST_SAMPLE)

threes = (path/'train'/'3').ls().sorted() # we want to sort it to make sure we have a same order everytime we create this object.
sevens = (path/'train'/'7').ls().sorted()

threes

threes[1]

three_tensors = [tensor(Image.open(o)) for o in threes] # taking an element from sevens as o and passing it to tensor(Image.open(o))
seven_tensors = [tensor(Image.open(o)) for o in sevens] # List comprehensions: e.g., new_list = [f(o) for o in a_list if o>0].
type(three_tensors), type(three_tensors[0]), len(three_tensors), len(seven_tensors)

three_tensors[0]

stacked_sevens = torch.stack(seven_tensors).float()/255 # stack up individual tensors in a collection into a single tensor.
stacked_threes = torch.stack(three_tensors).float()/255 # Generally when images are floats, the pixel values are expected to be between 0 and 1

type(stacked_sevens)

stacked_sevens.shape

valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])
valid_3_tens = valid_3_tens.float()/255
valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])
valid_7_tens = valid_7_tens.float()/255
valid_3_tens.shape,valid_7_tens.shape # 1,010 3s and 1,028 7s

# we change 28 x 28 images into a vector (1 x 784), using "view"
 # we also concatenate images in the train set into a single tensor
 # -1 is a special parameter to view that means "make this axis as big as necessary to fit all the data"
train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)

train_x.shape # 6131 + 6265 images

# We need a label for each image. We'll use 1 for 3 and 0 for 7
# unsqueeze(1) makes the outcome as rank-2 tensor
train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)
train_y.shape

train_y

"""## Creating a Model and Parameters"""

# Now we need an (initially random) weight for every pixel (this is the *initialize* step in our seven-step process):
def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()

weights = init_params(28*28,1)

weights

weights.shape

# Ws in the equation y=w*x+b is called the weights, and the b is called the bias. Together, the weights and bias make up the parameters.
bias = init_params(1)

bias

params = weights, bias
params[0], params[1]

(train_x[0]*params[0]).sum() + params[1] # This is our model, that predicts whether it is 3 or 7.

def linear1_image(x): return ((x*params[0]).sum() + params[1])

an_outc = linear1_image(train_x[0])
an_outc

# If the outcome is greater than 0, than we will classify the image as 3
pred_3 = (an_outc > 0.0)
pred_3, pred_3.float()

an_answer = train_y[0]
an_answer

pred_3.float() == an_answer

activ = an_outc.sigmoid()
activ

pred_3 = (activ > 0.5)

pred_3.float() == an_answer

"""## The MNIST LOSS"""

# For 3 images, 1-activation is loss; For 7 images, activation is loss
loss = torch.where(an_answer==1, 1-activ, activ)
loss.item()

loss

torch.where(tensor(1)==tensor(0), tensor(1), tensor(0))

params[0].data[210:212], params[1].data

params[0].grad, params[1].grad

loss.backward()

params[0].grad[210:212], params[1].grad

lr = 1.0
params[0].data -= params[0].grad * lr
params[1].data -= params[1].grad * lr

params[0].data[210:212], params[1].data

# calculating loss using the updated parameters and compare the updated loss with the old one
activ_updated = linear1_image(train_x[0]).sigmoid()
loss_updated = torch.where(an_answer==1, 1-activ_updated, activ_updated)
loss, loss_updated

"""### Updating parameters using a mini-batch for an epoch"""

# A Dataset in PyTorch is required to return a tuple of (x,y)
# a zip function and list provides a simple way to get this
dset = list(zip(train_x,train_y))
dset[0]
x,y = dset[0]
x.shape,y.shape

# A DataLoader can be created from a Dataset:
dl = DataLoader(dset, batch_size=256) # dset = list(zip(train_x,train_y)); 12,396 images
xb,yb = first(dl)

xb.shape

((xb@params[0]+params[1]).sigmoid()).shape

xb[:5,:3]

(xb[:5,:3]@params[0][:3]).reshape(-1,1)

weights = init_params(28*28)
bias = init_params(1)
params = weights, bias

def linear1_batch(xb): return (xb@params[0] + params[1]).reshape(-1,1)

outcs = linear1_batch(xb)
outcs.shape

activs = outcs.sigmoid()
activs.shape

torch.where(yb==1, 1-activs, activs).mean()

loss = torch.where(yb==1, 1-activs, activs).mean()
loss

loss.backward()
params[0].grad[210:212], params[1].grad

params[0].data[210:212], params[1].data

lr = 1
params[0].data -= params[0].grad * lr
params[1].data -= params[1].grad * lr

params[0].data[210:212], params[1].data

def mnist_loss(outcomes, targets):
    activs = outcomes.sigmoid() # applying the sigmoid function to the outcomes
    return torch.where(targets==1, 1-activs, activs).mean()
# torch.where(a,b,c): [b[i] if a[i] else c[i] for i in range(len(a))]
# Use list comprehension or torch.where, instead looping.
# Looping over tensors in Python performs at Python speed, not C/CUDA speed
# The loss is average value of the discrepancy for several images (i.e., mini-batch)

outcs = linear1_batch(xb)
loss = mnist_loss(outcs, yb)
loss.backward()
lr = 1.0
params[0].data -= params[0].grad * lr
params[1].data -= params[1].grad * lr

outcs_updated = linear1_batch(xb)
loss_updated = mnist_loss(outcs_updated, yb)
loss, loss_updated

for xb,yb in dl:
  outcs = linear1_batch(xb)
  loss = mnist_loss(outcs, yb)
  print("loss:" + str(loss.data.item()))
  loss.backward()
  params[0].data -= params[0].grad * lr
  params[1].data -= params[1].grad * lr

def train_epoch(dl, model, lr):
    for xb,yb in dl:
        outcs = model(xb)
        loss = mnist_loss(outcs, yb)
        print("loss:" + str(loss.data))
        loss.backward()
        for p in params:
            p.data -= p.grad*lr # specifying we are updating the values of our weights not the gradients of them
            p.grad.zero_() # we set the current gradients to 0

def calc_grad(xb, yb, model):
    outcs = model(xb)
    loss = mnist_loss(outcs, yb)
    #print(loss.data)
    loss.backward()

def train_epoch(dl, model, lr):
    for xb,yb in dl:
        calc_grad(xb, yb, model)
        for p in params:
            p.data -= p.grad*lr
            p.grad.zero_()

# That's our starting point. Let's train for one epoch, and see if the accuracy improves:
lr = 1.
params = weights,bias
train_epoch(dl, linear1_batch, lr)

"""## Calculating accuracies using the validation set"""

# That's our starting point. Let's train for one epoch, and see if the accuracy improves:
lr = 0.5
params = weights,bias
train_epoch(dl, linear1_batch, lr)

# We do same for the validation set
valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)
valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)
valid_dset = list(zip(valid_x,valid_y))
valid_dl = DataLoader(valid_dset, batch_size=256)

def batch_accuracy(outcomes, yb):
    outcs.sigmoid = outcomes.sigmoid()
    correct = (outcs.sigmoid > 0.5) == yb
    return correct.float().mean()

def validate_epoch(model):
    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]
    return round(torch.stack(accs).mean().item(), 4)

validate_epoch(linear1_batch)

# Then do a few more:
for i in range(20):
    train_epoch(dl, linear1_batch, lr)
    print(validate_epoch(linear1_batch), end=' ')

"""## Creating a Model and an Optimizer using PyTorch nn.Linear Module"""

# Using PyTorch's nn.Linear module instead of the linear1 function
# nn.Linear contains both the weights and biases in a single class
linear_model = nn.Linear(28*28,1)

# Every PyTorch module knows what parameters it has that can be trained
# they are available through the parameters method:
w,b = linear_model.parameters()
w.shape,b.shape

# We can use this information to create an optimizer:
class BasicOptim:
    def __init__(self,params,lr): self.params,self.lr = list(params),lr

    def step(self, *args, **kwargs):
        for p in self.params: p.data -= p.grad.data * self.lr

    def zero_grad(self, *args, **kwargs):
        for p in self.params: p.grad = None

# We can create our optimizer by passing in the model's parameters:
opt = BasicOptim(linear_model.parameters(), lr)

# Our training loop can now be simplified to:
def train_epoch(model):
    for xb,yb in dl:
        calc_grad(xb, yb, model)
        opt.step()
        opt.zero_grad()

# the same validation function
validate_epoch(linear_model)

# Let's put our little training loop in a function, to make things simpler:
def train_model(model, epochs):
    for i in range(epochs):
        train_epoch(model)
        print(validate_epoch(model), end=' ')

# The results are the same as in the previous section:
train_model(linear_model, 20)

#fastai provides the SGD class which, by default, does the same thing as our BasicOptim:
linear_model = nn.Linear(28*28,1)
opt = SGD(linear_model.parameters(), lr)
train_model(linear_model, 20)

# fastai also provides Learner.fit, which we can use instead of train_model.
# To create a Learner we first need to create a DataLoaders, by passing in our training and validation DataLoaders:
dls = DataLoaders(dl, valid_dl)

# To create a Learner without using an application (such as cnn_learner) we need to pass in all the elements that we've created in this chapter:
# the DataLoaders, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print:
learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)

# Now we can call fit:
learn.fit(10, lr=lr)

"""## Adding a Nonlinearity"""

def simple_net(xb):
    res = xb@w1 + b1
    res = res.max(tensor(0.0)) # rectified linear unit, also known as ReLU; in other words, replace every negative number with a zero
    res = res@w2 + b2
    return res

# Here, w1 and w2 are weight tensors, and b1 and b2 are bias tensors;
w1 = init_params((28*28,30))
b1 = init_params(30)
w2 = init_params((30,1))
b2 = init_params(1)

plot_function(F.relu)

# we can replace this code with something a bit simpler, by taking advantage of PyTorch:
simple_net = nn.Sequential(
    nn.Linear(28*28,30),
    nn.ReLU(),
    nn.Linear(30,1)
)
# nn.Sequential creates a module that will call each of the listed layers or functions in turn.
# nn.ReLU is a PyTorch module that does exactly the same thing as the F.relu function.
# Since modules are classes, we have to instantiate them, which is why you see nn.ReLU() in this example.

dls = DataLoaders(dl, valid_dl)

learn = Learner(dls, simple_net, opt_func=SGD,
                loss_func=mnist_loss, metrics=batch_accuracy)

#hide_output
learn.fit(40, 0.1)

plt.plot(L(learn.recorder.values).itemgot(2)); # accuracies over training

"""### Going Deeper"""

# an 18-layer model using the same approach
dls = ImageDataLoaders.from_folder(path)
learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy)
learn.fit_one_cycle(1, 0.1)

learn.model